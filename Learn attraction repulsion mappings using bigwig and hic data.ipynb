{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook takes Hi-C interaction data in an IJV format and chromatin data in .bigwig format and learns, for each chromosome, an attraction-repulsion using all other chromosomes that it then uses to predict an interaction map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#files are fed in as directories and prefixes as for speed we want a IJV or triplet or COO or coordinate \n",
    "#format file of interactions for every chromosome\n",
    "celltype = \"HCT116-untreated\"\n",
    "hic_directory = \"/Zulu/mike/dumped-hic/HCT116/\"\n",
    "hic_prefix = \"HCT116-untreated-q30-KR-dumped\"\n",
    "\n",
    "male = True #whether the .bigwig signals on the X chromosome should be doubled\n",
    "\n",
    "sizefile = \"/Zulu/mike/hg19.chrom.sizes\" #genome size file\n",
    "\n",
    "resolution = 100000 #resolution of the analysis\n",
    "small_resolution = 100000 #resolution of bigwig signals if you want it to be finer\n",
    "\n",
    "distance_min = 0 #minimum distance below which not to simulate\n",
    "distance_min_bins = int(distance_min/resolution)\n",
    "distance_max = 1000000000 #maximum distance above which not to simulate\n",
    "distance_max_bins = int(distance_max/resolution)\n",
    "\n",
    "chrstart = 0 #the index of the first chromosome to simulate\n",
    "chrstop = 23 #the index of the last chromosome to simulate\n",
    "\n",
    "tolerance = 10000 #tolerance and maximum iterations for the gradient descent\n",
    "max_iter = 10\n",
    "\n",
    "#these are thresholds used for excluding bins of each chromosomfe from the analysis for various reasons\n",
    "\n",
    "#100kb\n",
    "if resolution == 100000:\n",
    "    correction = \"sums nonzero medians quant\"\n",
    "    medianhighstd = 3\n",
    "    medianlowstd= 3\n",
    "    highstd = 3.5\n",
    "    lowstd = 2.5\n",
    "    nz_low_threshold = 3\n",
    "    nz_high_threshold = 3\n",
    "    zscore_threshold = 10\n",
    "\n",
    "#25kb\n",
    "elif resolution == 25000:\n",
    "    correction = \"sums nonzero\"\n",
    "    medianhighstd = 3\n",
    "    medianlowstd= 3\n",
    "    highstd = 5\n",
    "    lowstd = 5\n",
    "    nz_low_threshold = 3\n",
    "    nz_high_threshold = 3\n",
    "    zscore_threshold = 25\n",
    "else:\n",
    "    print(\"not a supported resolution, manually specify thresholds.\")\n",
    "\n",
    "ignorebins = [False] #the bins which will be excluded from analysis initialized to nothing.\n",
    "\n",
    "\n",
    "#signal files\n",
    "filenames = []\n",
    "#signal names\n",
    "names = []\n",
    "filenames.append(\"hct116-H3K9me3-ENCFF402WZH.bigWig\")\n",
    "names.append(\"H3K9me3\")\n",
    "filenames.append(\"hct116-H3K27me3-ENCFF030SYQ.bigWig\")\n",
    "names.append(\"H3K27me3\")\n",
    "filenames.append(\"hct116-H3K27ac-ENCFF225QAB.bigWig\")\n",
    "names.append(\"H3K27ac\")\n",
    "\n",
    "comp_num = len(filenames) #number of compartmental forces being simulated\n",
    "\n",
    "ignoreplots = False #toggles whether plots will be generated by the exclusion threshold methods. \n",
    "#Useful for tuning the above thresholds\n",
    "\n",
    "tiles = 21 #the number of quantiles for use in the analysis.\n",
    "#increased quantiles decreases bias, but in practice introduces more noise and increases variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we read in the sizefile\n",
    "\n",
    "sizes = open(sizefile,'r')\n",
    "\n",
    "chrnames = []\n",
    "chrsizes = []\n",
    "for line in sizes:\n",
    "    li = line.split()\n",
    "    if li[0] == 'Y' or li[0] == 'MT':\n",
    "        continue\n",
    "    chrnames.append(li[0])\n",
    "    chrsizes.append(int(li[1]))\n",
    "\n",
    "sizes.close()\n",
    "\n",
    "print(chrnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#determine size of each chromosome in bins:\n",
    "chrsizebins = []\n",
    "\n",
    "for i in range(len(chrnames)):\n",
    "    s = int(math.ceil(chrsizes[i]/resolution + 1))\n",
    "    chrsizebins.append(s)\n",
    "    \n",
    "totalsize = sum(chrsizebins)\n",
    "print(totalsize)\n",
    "print(chrsizebins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we initialize several variables for \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "max_size = max(chrsizebins) #largest chromosome\n",
    "\n",
    "D = list(np.zeros(max_size)) #a vector to hold on to the distance normalization.\n",
    "\n",
    "M = [] #These are the attraction-repulsion mappings that the model will learn.\n",
    "for i in range(comp_num):\n",
    "    M.append(np.zeros((tiles,tiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here we read in the bigwig files and put them into dataframes\n",
    "\n",
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import math\n",
    "from os.path import commonprefix\n",
    "\n",
    "resolution_ratio = resolution//small_resolution\n",
    "\n",
    "dfs = []\n",
    "mean_dfs = []\n",
    "#lets make a pandas dataframe\n",
    "for chrom in chrnames[chrstart:chrstop]:\n",
    "    medi_dic = {}\n",
    "    mean_dic = {}\n",
    "    lastlength = 0\n",
    "    for name,filen in zip(names,filenames):\n",
    "        try:\n",
    "            bw = pyBigWig.open(\"/Zulu/mike/chips/\"+filen)\n",
    "        except RuntimeError:\n",
    "            print(\"Trouble opening {0}\".format(filen))\n",
    "        print(bw.chroms())\n",
    "        prefix = commonprefix(bw.chroms().keys())\n",
    "        #calculate number of bins\n",
    "        chromlength = bw.chroms(prefix+chrom)\n",
    "        binnum = int(math.ceil(chromlength/small_resolution))\n",
    "        smalls = bw.stats(prefix+chrom,nBins=binnum)\n",
    "        medi_vals = []\n",
    "        mean_vals = []\n",
    "        for pos in range(resolution_ratio,binnum,resolution_ratio):\n",
    "            chunk = np.array(smalls[pos-resolution_ratio:pos], dtype=np.float)\n",
    "            try:\n",
    "                medi_vals.append(np.median(chunk))\n",
    "                mean_vals.append(np.mean(chunk))\n",
    "            except TypeError:\n",
    "                print(\"TypeError at:\")\n",
    "                print(chunk)\n",
    "        \n",
    "        if lastlength:\n",
    "            medi_vals.extend([0,0,0,0,0,0,0,0,0,0])\n",
    "            medi_vals = medi_vals[:lastlength]\n",
    "            mean_vals.extend([0,0,0,0,0,0,0,0,0,0])\n",
    "            mean_vals = mean_vals[:lastlength]\n",
    "        lastlength = len(medi_vals)\n",
    "        medi_dic[name] = medi_vals\n",
    "        mean_dic[name] = mean_vals\n",
    "        \n",
    "    \n",
    "    dfs.append(pd.DataFrame(data=medi_dic).fillna(0))\n",
    "    mean_dfs.append(pd.DataFrame(data=mean_dic).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we double every signal on the X chromosome if male is true\n",
    "\n",
    "if male == True:\n",
    "    xindex = chrnames.index('X')\n",
    "    dfs[xindex] = dfs[xindex] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Here we convert the values from the bigwig files in quantiles. quantiles aren't evenly spaced across range,\n",
    "#instead each contains the same number of bins.\n",
    "\n",
    "quantiles = np.linspace(0,1,num=tiles,endpoint=True)\n",
    "\n",
    "signals = []\n",
    "\n",
    "#first we just have to construct a big concatenated dataframe so we can call quantile on it.\n",
    "#Then we just apply those quantiles to each individual chrom df.\n",
    "\n",
    "concatdf = pd.concat(dfs)\n",
    "\n",
    "#get rid of zeros, since we are using fold/control bigwigs zeros indicate unmappable regiosn we should exclude\n",
    "nozconcatdf = concatdf[concatdf.sum(axis=1) != 0]\n",
    "\n",
    "wg_quantiles = nozconcatdf.quantile(quantiles)\n",
    "\n",
    "concatdf = []\n",
    "\n",
    "#now we know the quantile thresholds we construct new columns in the dataframe with the new quantile normalized values\n",
    "for df in dfs:\n",
    "    newcolumns = []\n",
    "    for column in df.iloc[:,-comp_num:]:        \n",
    "        newcolumns.append([])\n",
    "        for r in df[column]:\n",
    "            place = wg_quantiles[column].searchsorted(r)#[0]\n",
    "            newcolumns[-1].append(place)\n",
    "        \n",
    "    for index,column in enumerate(df.iloc[:,:comp_num]):\n",
    "        df[column+\"-tiles\"] = pd.Series(newcolumns[index],dtype='int16')\n",
    "\n",
    "\n",
    "#let's export those quantiles to a file for use in predicting maps in the subsequent notebook.\n",
    "filename = \"{0}-small:{1}-actual:{2}-{3}\".format(celltype,small_resolution,resolution,\"wgtiles\")\n",
    "wgt = open(filename,'w')\n",
    "\n",
    "wg_quantiles.to_csv(wgt)\n",
    "\n",
    "wgt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu(X,B):\n",
    "    v = np.ones(X.shape[0])\n",
    "    old = v + X @ B\n",
    "    return old\n",
    "\n",
    "\n",
    "def logL(Y,X,B):\n",
    "    \"\"\"\n",
    "    A function for computing the log likelihood given Y, X, and B\n",
    "    \"\"\"\n",
    "    u = mu(X,B)\n",
    "    return np.nansum((Y - u)**2)\n",
    "    \n",
    "\n",
    "def grad(Y,X,B):\n",
    "    \"\"\"\n",
    "    A function for computing the gradient given Y, X, and B\n",
    "    \"\"\"\n",
    "    u = mu(X,B)\n",
    "    return -2 * X.T @ (Y - u)\n",
    "\n",
    "def hess(Y,X,B):\n",
    "    \"\"\"\n",
    "    A function for computing the Hessian given Y, X, and B\n",
    "    \"\"\"\n",
    "    return (-2*X.T @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MLE_ncomp(chrindex,resolution=resolution,distance_min=distance_min,comp_num=comp_num,tiles=tiles,\n",
    "                     distance_min_bins=distance_min_bins,distance_max=distance_max,\n",
    "                    distance_max_bins=distance_max_bins,chrnames=chrnames,D=D,M=M,\n",
    "                     correction=correction,\n",
    "                     chrsizebins=chrsizebins,ignoreplots=ignoreplots,tolerance=tolerance,\n",
    "                     max_iter=max_iter,hic_directory=hic_directory,hic_prefix=hic_prefix,\n",
    "                     ignorebins=ignorebins,dfs=dfs,chrstart=chrstart,chrstop=chrstop,\n",
    "                     medianhighstd=medianhighstd,medianlowstd=medianlowstd,highstd=highstd,lowstd=lowstd,\n",
    "                      nz_low_threshold=nz_low_threshold,nz_high_threshold=nz_high_threshold):\n",
    "    \"\"\"\n",
    "    This is the main method for learning the attraction-repulsion mapping for each bigwig encoded signal to the interaction maps.\n",
    "    It begins by exlcuding bad bins from the analysis using the defined thresholds. It then loads in\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.sparse import coo_matrix\n",
    "    from scipy.sparse import csr_matrix\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "    \n",
    "    chrom1 = chrnames[chrindex]\n",
    "    chrom2 = chrnames[chrindex]\n",
    "    sizebins = chrsizebins[chrindex]\n",
    "    print(\"Loading: {0} vs {1}\".format(chrom1,chrom2))\n",
    "    print(\"Size: {0}\".format(sizebins))\n",
    "    \n",
    "    interactions = open(hic_directory+hic_prefix+chrom1+\"_\"+chrom2+\"_\"+str(resolution)+\".txt\",'r') #open interaction file\n",
    "        \n",
    "    matrix = np.zeros((sizebins,sizebins)) #initialize empty matrix\n",
    "    \n",
    "    for line in interactions:\n",
    "        li = line.split()\n",
    "        left = int(li[0])//resolution\n",
    "        right = int(li[1])//resolution\n",
    "        score = float(li[2])\n",
    "        matrix[left,right] = np.nan_to_num(score)\n",
    "        matrix[right][left] = np.nan_to_num(score)\n",
    "        \n",
    "    \n",
    "    if \"medians\" in correction:\n",
    "        medians = np.median(matrix,axis=0)\n",
    "    \n",
    "        median_mean = np.mean(medians[medians != 0])\n",
    "        median_stddev = np.std(medians[medians != 0])\n",
    "        \n",
    "        print(\"Median Mean: {0}\".format(median_mean))\n",
    "        print(\"Median Std dev: {0}\".format(median_stddev))\n",
    "        high_line_data = np.array([median_mean+(median_stddev*medianhighstd) for i in range(len(medians))])\n",
    "        low_line_data = np.array([median_mean-(median_stddev*medianlowstd) for i in range(len(medians))])\n",
    "        if ignoreplots:\n",
    "            plt.plot(medians)\n",
    "            plt.plot(high_line_data, 'r--')\n",
    "            plt.plot(low_line_data, 'r--')\n",
    "            plt.show()\n",
    "        medianignorebins = [(median_mean+(median_stddev*medianhighstd) < c) \n",
    "                      or median_mean-(median_stddev*medianlowstd) > c for c in medians]\n",
    "    else:\n",
    "        medianignorebins = [False for c in range(len(matrix[0]))]        \n",
    "\n",
    "    \n",
    "    \n",
    "    if \"sums\" in correction:\n",
    "        sums = np.sum(matrix,axis=0) + np.sum(matrix,axis=1)\n",
    "        sum_mean = np.mean(sums[sums != 0])\n",
    "        sum_stddev = np.std(sums[sums != 0])\n",
    "        print(\"Sum Mean: {0}\".format(sum_mean))\n",
    "        print(\"Sum Std dev: {0}\".format(sum_stddev))\n",
    "        high_line_data = np.array([sum_mean+(sum_stddev*highstd) for i in range(len(sums))])\n",
    "        low_line_data = np.array([sum_mean-(sum_stddev*lowstd) for i in range(len(sums))])\n",
    "        sumymin = sum_mean+(sum_stddev*highstd*5)\n",
    "        sumymax = sum_mean-(sum_stddev*lowstd*5)\n",
    "        \n",
    "        if ignoreplots:\n",
    "            plt.plot(sums)\n",
    "            plt.ylim(sumymin,sumymax)\n",
    "            plt.plot(high_line_data, 'r--')\n",
    "            plt.plot(low_line_data, 'r--')\n",
    "            plt.show()\n",
    "        sumignorebins = [(sum_mean+(sum_stddev*highstd) < c) or sum_mean-(sum_stddev*lowstd) > c for c in sums]\n",
    "    else:\n",
    "        sumignorebins = [False for c in range(len(matrix[0]))]\n",
    "        \n",
    "    \n",
    "    if \"nonzero\" in correction:\n",
    "        nonzerocounts = np.count_nonzero(matrix,axis=0)\n",
    "        nonzero_mean = np.mean(nonzerocounts[nonzerocounts != 0])\n",
    "        nonzero_stddev = np.std(nonzerocounts[nonzerocounts != 0])\n",
    "        print(\"Nonzero Mean: {0}\".format(nonzero_mean))\n",
    "        print(\"Nonzero Std dev: {0}\".format(nonzero_stddev))\n",
    "        nonzeroignorebins = [(nonzero_mean+(nonzero_stddev*nz_high_threshold) < c) or\n",
    "                      (nonzero_mean-(nonzero_stddev*nz_low_threshold) > c) for c in nonzerocounts]    \n",
    "        if ignoreplots:\n",
    "            plt.plot(nonzerocounts)\n",
    "            high_line_data = np.array([nonzero_mean+(nonzero_stddev*nz_high_threshold) for i in range(len(nonzerocounts))])\n",
    "            low_line_data = np.array([nonzero_mean-(nonzero_stddev*nz_low_threshold) for i in range(len(nonzerocounts))])   \n",
    "            plt.plot(high_line_data, 'r--')\n",
    "            plt.plot(low_line_data, 'r--')\n",
    "            plt.show()        \n",
    "    else:\n",
    "        nonzeroignorebins = [False for c in range(len(matrix[0]))]  \n",
    "        \n",
    "    df = dfs[chrindex-chrstart]\n",
    "    \n",
    "    sigs = []\n",
    "    \n",
    "    #load in sigs so we can remove 0 quantiles\n",
    "    signames = df.columns[-comp_num:].tolist()\n",
    "    print(signames)\n",
    "    for index, row in df.iloc[:,-comp_num:].iterrows():\n",
    "        try:\n",
    "            sigs.append([int(r) for r in row][-comp_num:])\n",
    "        except ValueError:\n",
    "            print(\"NaN\")\n",
    "            print(index)\n",
    "            print(row)  \n",
    "    \n",
    "    if \"quant\" in correction:\n",
    "        quantignorebins = [False for c in range(len(matrix[0]))]\n",
    "        for n in range(len(signames)):\n",
    "            print()\n",
    "            for x in range(len(sigs)):\n",
    "                if sigs[x][n] == 0:\n",
    "                    quantignorebins[x] = True\n",
    "    else:\n",
    "        quantignorebins = [False for c in range(len(matrix[0]))]      \n",
    "    \n",
    "    ignorebins = [w | x | y | z for (w,x,y,z) in zip(medianignorebins, sumignorebins, nonzeroignorebins,quantignorebins)]\n",
    "    print(\"Ignoring {0} median bins\".format(sum(medianignorebins)))\n",
    "    print(\"Ignoring {0} sum bins\".format(sum(sumignorebins)))\n",
    "    print(\"Ignoring {0} zero bins\".format(sum(nonzeroignorebins)))\n",
    "    print(\"Ignoring {0} quant bins\".format(sum(quantignorebins)))\n",
    "\n",
    "    #now we need to ignore z-norm outliers\n",
    "    #first we set ignore rows and columns to nan\n",
    "    #so they will be omitted by zscore operation\n",
    "    nan_ignore_matrix = np.copy(matrix)\n",
    "    nan_ignore_matrix[ignorebins,:] = np.nan\n",
    "    nan_ignore_matrix[:,ignorebins] = np.nan\n",
    "    \n",
    "    \n",
    "    zscoreignorebins = np.zeros(sizebins, dtype=bool)\n",
    "    for d in range(sizebins):\n",
    "        zscores = stats.zscore(np.diag(nan_ignore_matrix,d),nan_policy='omit')\n",
    "        for e,z in enumerate(zscores):\n",
    "            if z == np.nan:\n",
    "                zscoreignorebins[e] = False\n",
    "                zscoreignorebins[e+d] = False\n",
    "            elif z > zscore_threshold:\n",
    "                #make both row and column false\n",
    "                zscoreignorebins[e] = True\n",
    "                zscoreignorebins[e+d] = True\n",
    "                \n",
    "    #now we remove these bad bins\n",
    "    print(\"Ignoring {0} zscore bins\".format(sum(zscoreignorebins)))\n",
    "    ignorebins = [x | y for (x,y) in zip(ignorebins,zscoreignorebins)]\n",
    "    \n",
    "    print(\"Ignoring {0} total bins\".format(sum(ignorebins)))\n",
    "    #now matrix is our entirely unnormalized matrix\n",
    "    #initialize our variables:\n",
    "    dists = list(D[:sizebins])\n",
    "    maps = list(M[:])\n",
    "    \n",
    "    #here we want to set True on ignore flag for any bins outside sig coverage\n",
    "    if len(sigs) < len(ignorebins):\n",
    "        for x in range(len(sigs),len(ignorebins)):\n",
    "            ignorebins[x] = True\n",
    "    \n",
    "    LLL = np.Inf\n",
    "    count = 0 #intialize count at 0 for iterations\n",
    "    error = 100000 #initialize error at arbitrary high value to pass first iteration\n",
    "    \n",
    "    #OK now let's define all our matrices:\n",
    "    #to get y we need to compress to remove ignore bins, flatten,\n",
    "    igno = np.where(ignorebins)[0]\n",
    "    matrixdf = pd.DataFrame(matrix)\n",
    "\n",
    "    matrixdf.iloc[:,ignorebins] = np.nan\n",
    "    matrixdf.iloc[ignorebins,:] = np.nan\n",
    "    #now nan out the lower triangle:\n",
    "    matrixdf = matrixdf.where(np.tril(np.triu(np.ones(matrixdf.shape),k=1+distance_min_bins),k=min(distance_max_bins,matrixdf.shape[0])).astype(np.bool))\n",
    "    matrixdf = matrixdf.stack().reset_index()\n",
    "    matrixdf.columns = ['i','j','v']\n",
    "    Y = matrixdf['v'] #the vector of observed values from the interaction map    \n",
    " \n",
    "    #B is now the learned coefficients. Basically the flattened maps. Constructed by comparing sigs to y\n",
    "    ttiles = tiles-1\n",
    "    trisize = ((ttiles*(ttiles+1))//2)\n",
    "    bsize = trisize*comp_num\n",
    "    B = np.zeros(bsize)    \n",
    "    \n",
    "    #lastly we need to define H. This is the expected value based on distance for every value.\n",
    "    #we can initialize it as 0's and then learn it first.\n",
    "    H = np.zeros(Y.shape[0])\n",
    "    #to learn H you learn the full matrix, then you average every distance together\n",
    "    H_dists = matrixdf['j'] - matrixdf['i']\n",
    "    Hadj = np.ones(len(dists))\n",
    "    Hadj_exp = np.ones(Y.shape[0])\n",
    "    \n",
    "    #X is a CSR sparse matrix with each column corresponding to a sig-tile and each row a matrix bin.\n",
    "    #X dimensions are y x B\n",
    "    #once we have flattened y we can fill in each element in X. Should be 4 1's per row \n",
    "    #we construct by iterating over matrixdf, grabbing the appropriate sigs and adding that to a COO or DOK matrix.\n",
    "    xijdic = {}\n",
    "    for x in range(comp_num):\n",
    "        for i in range(ttiles):\n",
    "            for j in range(i,ttiles):\n",
    "                y = trisize*x + np.sum(np.array(range(-1*i,0))+ttiles+1) + j-i\n",
    "                xijdic[y] = [x,i,j]\n",
    "    \n",
    "    \n",
    "    #here we construct the sparse matrix X such that it is 0 in every bin except for the bins\n",
    "    #that map from a attraction-repulsion mapping weight to a chromosome bin.\n",
    "    ii = []\n",
    "    jj = []\n",
    "    dd = []\n",
    "    for index, row in matrixdf.iterrows():\n",
    "        sigs_i = sigs[int(row['i'])]\n",
    "        sigs_j = sigs[int(row['j'])]\n",
    "        for x, (i,j) in enumerate(zip(sigs_i,sigs_j)):\n",
    "            #here x is comp num, i is first quantile, j is second quantile\n",
    "            #formula to get correct bin therefore is (1+tiles*tiles*x + tiles*i + j)\n",
    "            if i > j:\n",
    "                h = i\n",
    "                i = j\n",
    "                j = h\n",
    "            ii.append(index)\n",
    "            i = i-1\n",
    "            j = j-1\n",
    "            y = trisize*x + np.sum(np.array(range(-1*i,0))+ttiles+1) + j-i\n",
    "            jj.append(y)\n",
    "            dd.append(1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    coo = coo_matrix((dd, (ii, jj)), shape=(Y.shape[0], bsize))\n",
    "    \n",
    "    X = csr_matrix(coo)\n",
    "    \n",
    "    #computing the average distance decay vector\n",
    "    for d in range(1, len(dists)):\n",
    "        #d_indices = np.where(H_dists == d)\n",
    "        dmean = np.mean(Y[H_dists == d])\n",
    "        H[H_dists == d] = dmean\n",
    "        dists[d] = dmean\n",
    "\n",
    "    #Plotting the distance decay vector\n",
    "    plt.plot(dists)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.title(\"Distance Vector {0}\".format(chrindex))\n",
    "    plt.show()\n",
    "    \n",
    "    #Sanity printing the shapes of each variable\n",
    "    print(\"X {0}\".format(X.shape))\n",
    "    print(\"Y {0}\".format(Y.shape))\n",
    "    print(\"B {0}\".format(B.shape))\n",
    "    print(\"H {0}\".format(H.shape))\n",
    "\n",
    "    #now we will begin iterating\n",
    "    while error > tolerance and count < max_iter:\n",
    "        count += 1\n",
    "        \n",
    "        #here we adjust the distance normalization according to the average attraction-repulsion mapping weight at each distance.\n",
    "        for d in range(1,len(dists)):\n",
    "            XBmean = np.mean(mu(X,B)[H_dists==d])\n",
    "            Hadj_exp[H_dists==d] = XBmean\n",
    "            Hadj[d] = XBmean\n",
    "        \n",
    "        #now divide by H\n",
    "        Yh = np.nan_to_num(Y/(H / Hadj_exp))\n",
    "        \n",
    "        #now we update B\n",
    "        lamb = 0.1\n",
    "\n",
    "        Grad = grad(Yh,X,B) #compute Gradient\n",
    "\n",
    "        Hess = hess(Yh,X,B).todense() #compute Hessian\n",
    "        \n",
    "        lamb = 1/max(np.absolute(Grad))\n",
    "        \n",
    "        inv = np.linalg.pinv(Hess) #inverse the Hessian\n",
    "        \n",
    "        step = -1 * (inv @ Grad[:, np.newaxis]) #compute the step using the inverted Hessian and the Gradient\n",
    "        \n",
    "        B_new = B[:, np.newaxis] - step\n",
    "        \n",
    "        B = np.array(B_new).reshape(B.shape[0])\n",
    "\n",
    "        #here we should reshape B to visualize as 4 heatmaps.\n",
    "        #inverse of this operation: 1+trisize*x + np.sum(np.array(range(-1*i,0))+tiles+1) + j\n",
    "        G_reshape = np.zeros((comp_num,ttiles,ttiles))\n",
    "        for n,g in enumerate(Grad):\n",
    "            x,i,j = xijdic[n]\n",
    "            G_reshape[x,i,j] = g\n",
    "            G_reshape[x,j,i] = g\n",
    "            \n",
    "        S_reshape = np.zeros((comp_num,ttiles,ttiles))\n",
    "        for n,s in enumerate(step):\n",
    "            x,i,j = xijdic[n]\n",
    "            S_reshape[x,i,j] = s\n",
    "            S_reshape[x,j,i] = s\n",
    "        \n",
    "        #reshape the weights (B) into squares\n",
    "        B_reshape = np.zeros((comp_num,ttiles,ttiles))\n",
    "        for n,b in enumerate(B):\n",
    "            x,i,j = xijdic[n]\n",
    "            B_reshape[x,i,j] = b\n",
    "            B_reshape[x,j,i] = b\n",
    "\n",
    "        #plot out the learned maps\n",
    "        for b,n in zip(B_reshape,signames):\n",
    "            sns.heatmap(b,center=0)\n",
    "            plt.title(\"Maps \"+ n)\n",
    "            plt.show()\n",
    "\n",
    "        #Log Likelihood\n",
    "        LL = logL(Yh,X,B)\n",
    "\n",
    "        print(\"Log likelihood: {0}\".format(LL))\n",
    "        print(\"Last log likelihood: {0}\".format(LLL))\n",
    "        error = LLL-LL\n",
    "        print(\"Chromosome {0} Iteration {1} Improvement: {2}\".format(chrom1,count,error))\n",
    "        LLL = LL\n",
    "        \n",
    "        expected = mu(X,B)\n",
    "        observed = Yh\n",
    "        expectedc = mu(X,B) * (H / Hadj_exp)\n",
    "        observedc  = Y\n",
    "        pearson = np.corrcoef(observed,expected)[0,1]\n",
    "        print('Pearson: (0)'.format(pearson))\n",
    "        \n",
    "        #generate some sample visualizations to see how well it worked.\n",
    "        \n",
    "        vis = np.zeros((sizebins,sizebins))\n",
    "        visc = np.zeros((sizebins,sizebins))\n",
    "        \n",
    "        for o,e,i,j in zip(observed,expected,matrixdf['i'],matrixdf['j']):\n",
    "            vis[i,j] = e\n",
    "            vis[j,i] = o\n",
    "        for o,e,i,j in zip(observedc,expectedc,matrixdf['i'],matrixdf['j']):\n",
    "            visc[i,j] = e\n",
    "            visc[j,i] = o            \n",
    "            \n",
    "        matplotlib.rcParams['figure.figsize'] = (12,12)\n",
    "        plt.imshow(vis)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(vis[100:200,100:200])\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        matplotlib.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "        matplotlib.rcParams['figure.figsize'] = (12,12)\n",
    "        plt.imshow(visc,vmin=0,vmax=1000)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(visc[100:200,100:200],vmin=0,vmax=1000)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        matplotlib.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "        maps = B_reshape\n",
    "        adj_dists = dists/Hadj\n",
    "    return (maps, sigs, adj_dists, ignorebins)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we run the above script to learn on each chromosome\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "msdi = []\n",
    "for x in range(chrstart,chrstop):\n",
    "    msdi.append(MLE_ncomp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we unpack the various features MLE_ncomp returned with\n",
    "mappings = []\n",
    "signals = []\n",
    "distances = []\n",
    "ignored = []\n",
    "\n",
    "for chromosome in msdi:\n",
    "    mappings.append(chromosome[0])\n",
    "    signals.append(chromosome[1])\n",
    "    distances.append(chromosome[2])\n",
    "    ignored.append(chromosome[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here we export the learned attraction-repulsion maps, distance normalization, and ignored bins to file.\n",
    "\n",
    "dfnames = []\n",
    "for c in dfs[0]:\n",
    "    dfnames.append(c)\n",
    "\n",
    "#here let's export tile thresholds and mappings to file\n",
    "for chrindex in range(chrstart,chrstop):\n",
    "    corrindex = chrindex - chrstart\n",
    "    for x,signame in enumerate(dfnames[-comp_num:]):\n",
    "        mapfileoutname = \"/Zulu/mike/learnedmaps/{0}-{1}kb-chr{2}-{3}-learnedmaps-min:max-{4}kb:{5}kb.csv\".format(celltype,resolution//1000,chrnames[chrindex],signame,distance_min//1000,distance_max//1000)\n",
    "\n",
    "        mapdf = pd.DataFrame(mappings[corrindex][x])\n",
    "        mapdf.to_csv(mapfileoutname,index=False)\n",
    "        print(mapfileoutname)\n",
    "        \n",
    "    distancefileoutname = \"/Zulu/mike/learneddistances/{0}-{1}kb-chr{2}-distances-min:max-{3}kb:{4}kb.csv\".format(celltype,resolution//1000,chrnames[chrindex],distance_min//1000,distance_max//1000)\n",
    "    distancedf = pd.DataFrame(distances[chrindex])\n",
    "    distancedf.to_csv(distancefileoutname,index=False,na_rep=0)\n",
    "    ignorefileoutname = \"/Zulu/mike/learnedignores/{0}-{1}kb-chr{2}-ignore.csv\".format(celltype,resolution//1000,chrnames[chrindex])\n",
    "    ignoredf = pd.DataFrame(ignored[corrindex])\n",
    "    ignoredf.to_csv(ignorefileoutname,index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
