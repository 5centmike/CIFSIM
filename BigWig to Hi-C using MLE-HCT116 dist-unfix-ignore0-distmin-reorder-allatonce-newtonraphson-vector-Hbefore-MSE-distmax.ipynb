{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Formula is simple Eij = Dij * (1 + Aij + Bij + Cij) where Aij is the ith and jth entry from the A matrix which we learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So what we need to do is read in each chromosome.\n",
    "#initialize all of our starting values.\n",
    "#iterate over the entire chromosome and update distance\n",
    "#iterate over the entire chromosome and update bias\n",
    "#iterate again and update C-score\n",
    "#repeat until error tolerance is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "#matplotlib.rcParams['figure.figsize'] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "celltype = \"HCT116-untreated\"\n",
    "#hic_directory = \"/Zulu/mike/dumped-hic/GM12878/\"\n",
    "hic_directory = \"/Zulu/mike/dumped-hic/HCT116/\"\n",
    "#hic_prefix = \"GM12878-untreated-NOKR-dumped\"\n",
    "#hic_prefix = \"GM12878-untreated-q30-KR-dumped\"\n",
    "hic_prefix = \"HCT116-untreated-q30-KR-dumped\"\n",
    "\n",
    "male = True\n",
    "\n",
    "sizefile = \"/Zulu/mike/hg19.chrom.sizes\"\n",
    "\n",
    "resolution = 100000\n",
    "small_resolution = 100000\n",
    "\n",
    "distance_min = 0#5000000\n",
    "distance_min_bins = int(distance_min/resolution)\n",
    "distance_max = 1000000000\n",
    "distance_max_bins = int(distance_max/resolution)\n",
    "#print(distance_min_bins)\n",
    "distance_expand_rate = 0.04\n",
    "\n",
    "chrstart = 0\n",
    "chrstop = 23\n",
    "\n",
    "tolerance = 10000\n",
    "max_iter = 10\n",
    "\n",
    "ignore_threshold = 0.01\n",
    "\n",
    "#100kb\n",
    "if resolution == 100000:\n",
    "    correction = \"sums nonzero medians quant\"\n",
    "    medianhighstd = 3\n",
    "    medianlowstd= 3\n",
    "    highstd = 3.5\n",
    "    lowstd = 2.5\n",
    "    nz_low_threshold = 3\n",
    "    nz_high_threshold = 3\n",
    "    zscore_threshold = 10\n",
    "\n",
    "#25kb\n",
    "elif resolution == 25000:\n",
    "    correction = \"sums nonzero\"\n",
    "    medianhighstd = 3\n",
    "    medianlowstd= 3\n",
    "    highstd = 5\n",
    "    lowstd = 5\n",
    "    nz_low_threshold = 3\n",
    "    nz_high_threshold = 3\n",
    "    zscore_threshold = 25\n",
    "else:\n",
    "    print(\"woah nelly bad resolution\")\n",
    "\n",
    "ignorebins = [False]\n",
    "\n",
    "\n",
    "#number of potential compartments\n",
    "#signal files\n",
    "filenames = []\n",
    "#signal names\n",
    "names = []\n",
    "filenames.append(\"hct116-H3K9me3-ENCFF402WZH.bigWig\")\n",
    "names.append(\"H3K9me3\")\n",
    "filenames.append(\"hct116-H3K27me3-ENCFF030SYQ.bigWig\")\n",
    "names.append(\"H3K27me3\")\n",
    "filenames.append(\"hct116-H3K27ac-ENCFF225QAB.bigWig\")\n",
    "names.append(\"H3K27ac\")\n",
    "#filenames.append(\"GSE86165_HCT116_gro.bw\")\n",
    "#names.append(\"Gro\")\n",
    "#filenames.append(\"hct116-CTCF-ENCFF695DDS.bigWig\")\n",
    "#names.append(\"CTCF\")\n",
    "#filenames.append(\"hct116-H2AFZ-ENCFF996RJO.bigWig\")\n",
    "#names.append(\"H2AFZ\")\n",
    "#filenames.append(\"hct116-H3K9me2-ENCFF854QMM.bigWig\")\n",
    "#names.append(\"H3K9me2\")\n",
    "#filenames.append(\"hct116-H3K4me2-ENCFF563OKQ.bigWig\")\n",
    "#names.append(\"H3K4me2\")\n",
    "#filenames.append(\"hct116-H3K79me2-ENCFF127XQD.bigWig\")\n",
    "#names.append(\"H3K79me2\")\n",
    "#filenames.append(\"hct116-H4K20me1-ENCFF355TAA.bigWig\")\n",
    "#names.append(\"H4K20me1\")\n",
    "#filenames.append(\"hct166-H3K4me1-ENCFF285DIL.bigWig\")\n",
    "#names.append(\"H3K4me1\")\n",
    "#filenames.append(\"hct116-H3K4me3-ENCFF057GFU.bigwig\")\n",
    "#names.append(\"H3K4me3-1\")\n",
    "#filenames.append(\"hct116-H3K4me3-ENCFF859AQD.bigwig\")\n",
    "#names.append(\"H3K4me3-2\")\n",
    "#filenames.append(\"hct116-H3K36me3-ENCFF062CBC.bigWig\")\n",
    "#names.append(\"H3K36me3\")\n",
    "\n",
    "comp_num = len(filenames)\n",
    "\n",
    "#smooth = 21\n",
    "\n",
    "fft_fc = 0.01\n",
    "fft_b = 0.08\n",
    "fft_N = int(np.ceil((4 / fft_b)))\n",
    "if not fft_N % 2: fft_N += 1\n",
    "fft_n = np.arange(fft_N)\n",
    "\n",
    "ignoreplots = False\n",
    "\n",
    "tiles = 21\n",
    "\n",
    "clims = [0,1]\n",
    "slims = [0,10]\n",
    "blims = [0,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X']\n"
     ]
    }
   ],
   "source": [
    "sizes = open(sizefile,'r')\n",
    "\n",
    "chrnames = []\n",
    "chrsizes = []\n",
    "for line in sizes:\n",
    "    li = line.split()\n",
    "    if li[0] == 'Y' or li[0] == 'MT':\n",
    "        continue\n",
    "    chrnames.append(li[0])\n",
    "    chrsizes.append(int(li[1]))\n",
    "\n",
    "sizes.close()\n",
    "\n",
    "print(chrnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30399\n",
      "[2494, 2433, 1982, 1913, 1811, 1713, 1593, 1465, 1414, 1357, 1352, 1340, 1153, 1075, 1027, 905, 813, 782, 593, 632, 483, 515, 1554]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "#determine size of each chromosome in bins:\n",
    "chrsizebins = []\n",
    "\n",
    "for i in range(len(chrnames)):\n",
    "    s = int(math.ceil(chrsizes[i]/resolution + 1))\n",
    "    chrsizebins.append(s)\n",
    "    \n",
    "totalsize = sum(chrsizebins)\n",
    "print(totalsize)\n",
    "print(chrsizebins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#inits\n",
    "max_size = max(chrsizebins)\n",
    "print(max_size)\n",
    "\n",
    "distance_indices = []#[0]*distance_min_bins\n",
    "distance_ranges = []\n",
    "#d = int(distance_min_bins)\n",
    "d = 0\n",
    "count = 0\n",
    "while d < max_size:\n",
    "    d1 = d\n",
    "    distance_ranges.append([d,d1])\n",
    "    distance_indices.extend([count]*(d1-d+1))\n",
    "    #print(len(distance_indices))\n",
    "    d = d1+1\n",
    "    #print(d1)\n",
    "    count += 1\n",
    "    \n",
    "\n",
    "#distance_range_count = len(distance_ranges)\n",
    "#print(distance_range_count)\n",
    "#print(distance_ranges)\n",
    "#print(len(distance_indices))\n",
    "#print(distance_indices[210:230])\n",
    "#D = list(np.random.rand(max_size) * 2)\n",
    "D = list(np.zeros(max_size))\n",
    "#S = list(np.random.rand(distance_range_count,comp_num))\n",
    "#S = list(np.random.rand(distance_range_count,comp_num) * 2)\n",
    "#S = list(np.ones((distance_range_count,comp_num)) * 2)\n",
    "#this is for each distance but you need to use chrsize, distance_min, and merge rate to calculate the actual size\n",
    "#init should follow a power law decay? slightly different because distances are expanding\n",
    "#B = [1]*max_size\n",
    "#B = list(np.random.rand(max_size) * 2)\n",
    "#this is bias for each bin\n",
    "#should start as 1\n",
    "#C = [0.01]*max_size\n",
    "#C = list(np.random.rand(max_size,comp_num))\n",
    "#C = []\n",
    "#this is C-score,\n",
    "#should start as 0, same size as B\n",
    "\n",
    "M = []\n",
    "for i in range(comp_num):\n",
    "    M.append(np.zeros((tiles,tiles)))\n",
    "    \n",
    "#print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hct116-H3K9me3-ENCFF402WZH.bigWig', 'hct116-H3K27me3-ENCFF030SYQ.bigWig', 'hct116-H3K27ac-ENCFF225QAB.bigWig']\n",
      "1\n",
      "{'chrM': 16571, 'chr21': 48129895, 'chr5': 180915260, 'chr9': 141213431, 'chr8': 146364022, 'chr1': 249250621, 'chr4': 191154276, 'chr7': 159138663, 'chr22': 51304566, 'chr19': 59128983, 'chr6': 171115067, 'chr14': 107349540, 'chr17': 81195210, 'chrY': 59373566, 'chr13': 115169878, 'chr18': 78077248, 'chr3': 198022430, 'chr2': 243199373, 'chr11': 135006516, 'chr20': 63025520, 'chr15': 102531392, 'chr16': 90354753, 'chr12': 133851895, 'chr10': 135534747, 'chrX': 155270560}\n",
      "2493\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2057b7e5d80f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mbinnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromlength\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msmall_resolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0msmalls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchrom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnBins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mmedi_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmean_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import math\n",
    "from os.path import commonprefix\n",
    "\n",
    "#small_resolution = 5000\n",
    "resolution_ratio = resolution//small_resolution\n",
    "\n",
    "\n",
    "print(filenames)\n",
    "#filenames = [\"hct116-H3K9me3-ENCFF402WZH.bigWig\",\"hct116-H3K27me3-ENCFF030SYQ.bigWig\",\"hct116-H3K27ac-ENCFF225QAB.bigWig\"]\n",
    "#names = [\"H3K9me3\",\"H3K27me3\",\"H3K27ac\"]\n",
    "dfs = []\n",
    "mean_dfs = []\n",
    "#lets make a pandas dataframe\n",
    "for chrom in chrnames[chrstart:chrstop]:\n",
    "    print(chrom)\n",
    "    medi_dic = {}\n",
    "    mean_dic = {}\n",
    "    lastlength = 0\n",
    "    for name,filen in zip(names,filenames):\n",
    "        #print(\"Reading in: {0}\".format(name))\n",
    "        try:\n",
    "            bw = pyBigWig.open(\"/Zulu/mike/chips/\"+filen)\n",
    "        except RuntimeError:\n",
    "            print(\"Trouble opening {0}\".format(filen))\n",
    "        print(bw.chroms())\n",
    "        prefix = commonprefix(bw.chroms().keys())\n",
    "        #print(prefix)\n",
    "        #calculate number of 25kb bins\n",
    "        chromlength = bw.chroms(prefix+chrom)\n",
    "        #print(chromlength)\n",
    "        binnum = int(math.ceil(chromlength/small_resolution))\n",
    "        print(binnum)\n",
    "        smalls = bw.stats(prefix+chrom,nBins=binnum)\n",
    "        medi_vals = []\n",
    "        mean_vals = []\n",
    "        for pos in range(resolution_ratio,binnum,resolution_ratio):\n",
    "            chunk = np.array(smalls[pos-resolution_ratio:pos], dtype=np.float)\n",
    "            try:\n",
    "                medi_vals.append(np.median(chunk))\n",
    "                mean_vals.append(np.mean(chunk))\n",
    "            except TypeError:\n",
    "                print(chunk)\n",
    "        \n",
    "        #print(chromlength)\n",
    "        #print(binnum)\n",
    "        #print(len(vals))\n",
    "        #x = small_resolution\n",
    "        #vals = []\n",
    "        #while x < chromlength:\n",
    "            #print(x)\n",
    "            #vals.extend(bw.stats(prefix+chrom,x-small_resolution,x))\n",
    "            #x += small_resolution\n",
    "        #vals.extend(bw.stats(prefix+chrom,x-small_resolution,chromlength))\n",
    "        #print(bw.stats(\"chr\"+chrom,x-resolution,chromlength))\n",
    "        #vals.extend([0,0,0,0,0])\n",
    "        \n",
    "        if lastlength:\n",
    "            medi_vals.extend([0,0,0,0,0,0,0,0,0,0])\n",
    "            medi_vals = medi_vals[:lastlength]\n",
    "            mean_vals.extend([0,0,0,0,0,0,0,0,0,0])\n",
    "            mean_vals = mean_vals[:lastlength]\n",
    "        lastlength = len(medi_vals)\n",
    "        #print(len(vals))\n",
    "        #print(vals[:10])\n",
    "        medi_dic[name] = medi_vals\n",
    "        mean_dic[name] = mean_vals\n",
    "        \n",
    "    \n",
    "    dfs.append(pd.DataFrame(data=medi_dic).fillna(0))\n",
    "    mean_dfs.append(pd.DataFrame(data=mean_dic).fillna(0))\n",
    "    \n",
    "for df in dfs:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ehre we need to double every signal on the X chromosome\n",
    "\n",
    "if male == True:\n",
    "    xindex = chrnames.index('X')\n",
    "    print(dfs[xindex])\n",
    "    dfs[xindex] = dfs[xindex] * 2\n",
    "    print(dfs[xindex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "hists = [[] for x in range(comp_num)]\n",
    "\n",
    "print(comp_num)\n",
    "count = 0\n",
    "for df in dfs:\n",
    "    count += 1\n",
    "    plt.plot(df[\"H3K9me3\"])\n",
    "    plt.title(count)\n",
    "    plt.show()\n",
    "    plt.hist(df[\"H3K9me3\"],bins=50)\n",
    "    plt.title(count)\n",
    "    plt.show()\n",
    "    for x,column in enumerate(df[:comp_num]):\n",
    "        #print(x)\n",
    "        hists[x].extend(df[column].tolist())\n",
    "    \n",
    "\n",
    "for n,h in zip(names,hists):\n",
    "    plt.hist(h,bins=50)\n",
    "    plt.title(n)\n",
    "    plt.show()\n",
    "    \n",
    "hists = [[] for x in range(comp_num)]    \n",
    "\n",
    "for df in mean_dfs:\n",
    "    #print(df)\n",
    "    for x,column in enumerate(df[:comp_num]):\n",
    "        #print(x)\n",
    "        hists[x].extend(df[column].tolist())\n",
    "    \n",
    "\n",
    "for n,h in zip(names,hists):\n",
    "    plt.hist(h,bins=50)\n",
    "    plt.title(n)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "#chrom = 1\n",
    "\n",
    "start = 0\n",
    "end = 1000\n",
    "\n",
    "#plt.plot(dfs[chrom][0:1000])\n",
    "#plt.plot(mean_dfs[chrom][0:1000][:comp_num])\n",
    "#plt.ylim(0,10)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we will now implement a low-pass filter using fourier transform\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "\n",
    "fft_fc = 0.1\n",
    "fft_b = 0.08\n",
    "fft_N = int(np.ceil((4 / fft_b)))\n",
    "if not fft_N % 2: fft_N += 1\n",
    "fft_n = np.arange(fft_N)\n",
    "\n",
    "\n",
    "sinc_func = np.sinc(2 * fft_fc * (fft_n - (fft_N - 1) / 2.))\n",
    "window = 0.42 - 0.5 * np.cos(2 * np.pi * fft_n / (fft_N - 1)) + 0.08 * np.cos(4 * np.pi * fft_n / (fft_N - 1))\n",
    "sinc_func = sinc_func * window\n",
    "sinc_func = sinc_func / np.sum(sinc_func)\n",
    "\n",
    "s = list(df[\"Gro\"])\n",
    "fft_signal = np.convolve(s, sinc_func)\n",
    "smooth_signal = list(df[\"Gro\"].rolling(window=smooth,min_periods=1,center=True).mean())\n",
    "\n",
    "xleft = 250\n",
    "xright = 350\n",
    "\n",
    "plt.plot(s[xleft:xright])\n",
    "plt.plot(fft_signal[xleft:xright])\n",
    "plt.plot(smooth_signal[xleft:xright])\n",
    "#plt.legend()\n",
    "plt.ylim(0,200)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#let's convert the fold changes over control to 'tiles. 'tiles aren't evenly spaced across range,\n",
    "#instead each contains the same number of entries.\n",
    "\n",
    "#this is quantiles, we want a more meaningful distribution though\n",
    "\n",
    "quantiles = np.linspace(0,1,num=tiles,endpoint=True)\n",
    "#print(quantiles)\n",
    "\n",
    "\n",
    "signals = []\n",
    "\n",
    "#first we just have to construct a big concatenated dataframe so we can call quantile on it.\n",
    "#Then we just apply those quantiles to each individual chrom df.\n",
    "\n",
    "concatdf = pd.concat(dfs)\n",
    "\n",
    "#it would be nice if this worked but it doesn't so whatever.\n",
    "\n",
    "print(concatdf.shape)\n",
    "print((concatdf.sum(axis=1)))\n",
    "\n",
    "\n",
    "#concatdf[(concatdf.sum(axis=1) != 0)]  \n",
    "nozconcatdf = concatdf[concatdf.sum(axis=1) != 0]\n",
    "\n",
    "#nozconcatdf = concatdf[(df.T != 0).any()]\n",
    "\n",
    "print(nozconcatdf.shape)\n",
    "\n",
    "wg_quantiles = nozconcatdf.quantile(quantiles)\n",
    "\n",
    "print(\"quantiles\")\n",
    "print(wg_quantiles)\n",
    "\n",
    "concatdf = []\n",
    "\n",
    "\n",
    "for df in dfs:\n",
    "    #df.replace(0, np.NaN,inplace=True)\n",
    "    #for col in df:\n",
    "        #col_smooth = col + '_smooth'\n",
    "        #df[col_smooth] = df[col].rolling(window=smooth,min_periods=1,center=True).mean()\n",
    "        #print(df[col])\n",
    "        #print(df[col_smooth])\n",
    "    \n",
    "    #print(df)\n",
    "    #print(\"q\")\n",
    "    #print(q)\n",
    "    newcolumns = []\n",
    "    for column in df.iloc[:,-comp_num:]:\n",
    "        #print(df[column])\n",
    "        #print(q[column])\n",
    "        \n",
    "        newcolumns.append([])\n",
    "        for r in df[column]:\n",
    "            place = wg_quantiles[column].searchsorted(r)#[0]\n",
    "            newcolumns[-1].append(place)\n",
    "        \n",
    "    for index,column in enumerate(df.iloc[:,:comp_num]):\n",
    "        df[column+\"-tiles\"] = pd.Series(newcolumns[index],dtype='int16')\n",
    "\n",
    "#todo, more sophisticated tiling that uses a different dynamic range for each signal based on it's variance.\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (5,5)\n",
    "\n",
    "for col in dfs[0].iloc[:,-comp_num:]:\n",
    "    df.hist(col,bins=range(tiles+1))\n",
    "    plt.show()\n",
    "    \n",
    "for col in dfs[0].iloc[:,:comp_num]:\n",
    "    df.hist(col,bins=200)\n",
    "    plt.show()\n",
    "    \n",
    "#for col in dfs[1].iloc[:,-comp_num:]:\n",
    "#    df.hist(col,bins=range(tiles+1))\n",
    "#    plt.show()\n",
    "\n",
    "#for x in range(quantiles):\n",
    "\n",
    "#print(dfs)\n",
    "\n",
    "\n",
    "#let's export those quantiles to a file\n",
    "filename = \"{0}-small:{1}-actual:{2}-{3}\".format(celltype,small_resolution,resolution,\"wgtiles\")\n",
    "wgt = open(filename,'w')\n",
    "\n",
    "wg_quantiles.to_csv(wgt)\n",
    "\n",
    "\n",
    "wgt.close()\n",
    "print(filename)\n",
    "! head -n 100 $filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(maps,sig1,sig2):\n",
    "    total = 1\n",
    "    for m,i,j in zip(maps,sig1,sig2):\n",
    "        total += m[i,j]\n",
    "    return total\n",
    "\n",
    "\n",
    "\n",
    "def ncomp_mapping_update(matrix,sigs,maps,tiles,dists,distance_indices,slims,ignorebins,distance_min_bins):\n",
    "    \n",
    "    #lamb = 0.1\n",
    "    \n",
    "    #we need to iterate over all pixels in the matrix, but we just need to hold on to sums.\n",
    "    import numpy as np\n",
    "    cnum = len(sigs[0])\n",
    "    mnum = len(matrix[0])\n",
    "    gradmaps = []\n",
    "    hessmaps = []\n",
    "    countmaps = []\n",
    "    newmaps = []\n",
    "    for c in range(cnum):\n",
    "        gradmaps.append(np.zeros((tiles,tiles)))\n",
    "        hessmaps.append(np.zeros((tiles,tiles)))\n",
    "        countmaps.append(np.zeros((tiles,tiles)))\n",
    "        #newmaps.append(np.ones((tiles,tiles)))\n",
    "        maps.append(np.array(maps[c]))\n",
    "    for i in range(mnum):\n",
    "        for j in range(i+distance_min_bins,mnum):\n",
    "            if ignorebins[i] or ignorebins[j]:\n",
    "                continue\n",
    "            #we are iterating over the entire matrix\n",
    "            distfactor = dists[distance_indices[j-i]]\n",
    "            for c in range(cnum):\n",
    "                #we are iterating over each signal or compartment\n",
    "                si = sigs[i][c]\n",
    "                sj = sigs[j][c]\n",
    "                #mapped = mapping(maps[0:c]+maps[c+1:cnum],sigs[i][0:c]+sigs[i][c+1:cnum],sigs[j][0:c]+sigs[j][c+1:cnum])\n",
    "                mapped = mapping(maps,sigs[i],sigs[j])\n",
    "                \n",
    "                o = np.nan_to_num(matrix[i][j])\n",
    "                 \n",
    "                grad = o/mapped - distfactor\n",
    "                \n",
    "                hess = -1 * o/(mapped*mapped)\n",
    "                \n",
    "                gradmaps[c][si,sj] += grad\n",
    "                gradmaps[c][sj,si] += grad\n",
    "                hessmaps[c][si,sj] += hess\n",
    "                hessmaps[c][sj,si] += hess                \n",
    "                \n",
    "                \n",
    "                #obsmaps[c][si,sj] += np.nan_to_num(matrix[i][j]/distfactor)\n",
    "                #expmaps[c][si,sj] += mapped\n",
    "                #obsmaps[c][sj,si] += np.nan_to_num(matrix[i][j]/distfactor)\n",
    "                #expmaps[c][sj,si] += mapped\n",
    "                #countmaps[c][si,sj] += 1\n",
    "                #countmaps[c][sj,si] += 1\n",
    "                #equat = (obsmaps[c][si,sj] - expmaps[c][si,sj])/countmaps[c][si,sj]\n",
    "                #newmaps[c][si,sj] = (obsmaps[c][si,sj] - expmaps[c][si,sj])/countmaps[c][si,sj]\n",
    "                #newmaps[c][sj,si] = (obsmaps[c][sj,si] - expmaps[c][sj,si])/countmaps[c][sj,si]\n",
    "    \n",
    "    for c in range(cnum):\n",
    "        step = gradmaps[c]/hessmaps[c]\n",
    "        #super_threshold_indices = hold < slims[0]\n",
    "        #hold[super_threshold_indices] = slims[0]\n",
    "        #add in a bit to deal with nans\n",
    "        hold2 = maps[c] - step\n",
    "        nan_indices = np.isnan(hold2)\n",
    "        hold2[nan_indices] = slims[0]\n",
    "        maps[c] = hold2\n",
    "    \n",
    "    #for c in range(cnum):\n",
    "    #    newmaps[c] = (obsmaps[c] - expmaps[c])/countmaps[c]\n",
    "        \n",
    "    return maps,countmaps#,weirds\n",
    "\n",
    "\n",
    "def ncomp_distance_update(d,matrix,sizebins,dists,sigs,maps,distance_ranges,ignorebins):\n",
    "    #we need to iterate over all pixels in this range\n",
    "    drange = distance_ranges[d]\n",
    "    observed = 0\n",
    "    expected = 0\n",
    "    for i in range(sizebins):\n",
    "        for j in range(i+drange[0],min(i+drange[1]+1,sizebins)):\n",
    "            if ignorebins[i] or ignorebins[j]:\n",
    "                continue\n",
    "            observed += matrix[i,j]\n",
    "            #this just happens to equate to the average since expected is 1 at beginning.\n",
    "            expected += mapping(maps,sigs[i],sigs[j])\n",
    "    #we now have observed and expected for every bin in range\n",
    "    if not observed and not expected:\n",
    "        #here we are out of range so just continue\n",
    "        return 0\n",
    "    return np.nan_to_num(observed/expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu(X,B):\n",
    "    v = np.ones(X.shape[0])\n",
    "    #print(v.shape)\n",
    "    #print(X.shape)\n",
    "    #print(B.shape)\n",
    "    #print((X @ B).shape)\n",
    "    old = v + X @ B\n",
    "    #print(old.shape)\n",
    "    #print(H.shape)\n",
    "    return old\n",
    "\n",
    "\n",
    "def logL(Y,X,B):\n",
    "    u = mu(X,B)\n",
    "    return np.nansum((Y - u)**2)\n",
    "    \n",
    "\n",
    "def grad(Y,X,B):\n",
    "    u = mu(X,B)\n",
    "    #v = np.ones(X.shape[0])\n",
    "    return -2 * X.T @ (Y - u)\n",
    "\n",
    "def hess(Y,X,B):\n",
    "    \"\"\"\n",
    "    from scipy.sparse import coo_matrix\n",
    "    from scipy.sparse import csr_matrix\n",
    "    u = mu(X,B)\n",
    "    num = (Y)\n",
    "    denom = (u*u)\n",
    "    frac = (num / denom)\n",
    "    #print('frac')\n",
    "    #print(frac.shape)\n",
    "    #refrac = frac[:, np.newaxis]\n",
    "    #print(refrac.shape)\n",
    "    #print(X.shape)\n",
    "    diag_i = []\n",
    "    diag_j = []\n",
    "    diag_v = []\n",
    "    for x,entry in enumerate(frac):\n",
    "        diag_i.append(x)\n",
    "        diag_j.append(x)\n",
    "        diag_v.append(entry)\n",
    "\n",
    "    coo_diag = coo_matrix((diag_v, (diag_i, diag_j)), shape=(Y.shape[0], Y.shape[0]))\n",
    "    #print(coo_diag)\n",
    "    csr_diag = csr_matrix(coo_diag)    \n",
    "    one = (csr_diag @ X)\n",
    "    \"\"\"\n",
    "    return (-2*X.T @ X)\n",
    "    \n",
    "    #return -1 * (((Y*H*H) / (u*u) @ X).T @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MLE_ncomp(chrindex,resolution=resolution,distance_min=distance_min,comp_num=comp_num,tiles=tiles,\n",
    "                     distance_min_bins=distance_min_bins,distance_max=distance_max,distance_max_bins=distance_max_bins,chrnames=chrnames,D=D,M=M,\n",
    "                     clims=clims,slims=slims,blims=blims,correction=correction,\n",
    "                     chrsizebins=chrsizebins,distance_ranges=distance_ranges,ignoreplots=ignoreplots,\n",
    "                     distance_indices=distance_indices,tolerance=tolerance,ignore_threshold=ignore_threshold,\n",
    "                     max_iter=max_iter,hic_directory=hic_directory,hic_prefix=hic_prefix,\n",
    "                     ignorebins=ignorebins,dfs=dfs,chrstart=chrstart,chrstop=chrstop,\n",
    "                     medianhighstd=medianhighstd,medianlowstd=medianlowstd,highstd=highstd,lowstd=lowstd,\n",
    "                      nz_low_threshold=nz_low_threshold,nz_high_threshold=nz_high_threshold):\n",
    "    #now we will iterate through each chromosome.\n",
    "    #for each chromosome we load in the matrix\n",
    "    #initialize all of the parameters\n",
    "    #for every bin i + distance_min/resolution >= j\n",
    "    #compute gradient for distance\n",
    "    #then compute gradient for bias\n",
    "    #then compute c-score\n",
    "    #then compute error\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.sparse import coo_matrix\n",
    "    from scipy.sparse import csr_matrix\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "    \n",
    "    chrom1 = chrnames[chrindex]\n",
    "    chrom2 = chrnames[chrindex]\n",
    "    sizebins = chrsizebins[chrindex]\n",
    "    print(\"Loading: {0} vs {1}\".format(chrom1,chrom2))\n",
    "    print(\"Size: {0}\".format(sizebins))\n",
    "    \n",
    "    interactions = open(hic_directory+hic_prefix+chrom1+\"_\"+chrom2+\"_\"+str(resolution)+\".txt\",'r')\n",
    "        \n",
    "    #matrix = dok_matrix((sizebins,sizebins))\n",
    "    matrix = np.zeros((sizebins,sizebins))\n",
    "    print(matrix)\n",
    "    \n",
    "    for line in interactions:\n",
    "        li = line.split()\n",
    "        left = int(li[0])//resolution\n",
    "        right = int(li[1])//resolution\n",
    "        score = float(li[2])\n",
    "        matrix[left,right] = np.nan_to_num(score)\n",
    "        matrix[right][left] = np.nan_to_num(score)\n",
    "        \n",
    "        \n",
    "    #plt.imshow(matrix,vmin = 0,vmax = 25)\n",
    "    #plt.show()\n",
    "    \n",
    "    if \"medians\" in correction:\n",
    "        medians = np.median(matrix,axis=0)\n",
    "    \n",
    "        median_mean = np.mean(medians[medians != 0])\n",
    "        median_stddev = np.std(medians[medians != 0])\n",
    "        \n",
    "        print(\"Median Mean: {0}\".format(median_mean))\n",
    "        print(\"Median Std dev: {0}\".format(median_stddev))\n",
    "        high_line_data = np.array([median_mean+(median_stddev*medianhighstd) for i in range(len(medians))])\n",
    "        low_line_data = np.array([median_mean-(median_stddev*medianlowstd) for i in range(len(medians))])\n",
    "        if ignoreplots:\n",
    "            plt.plot(medians)\n",
    "            plt.plot(high_line_data, 'r--')\n",
    "            plt.plot(low_line_data, 'r--')\n",
    "            plt.show()\n",
    "        medianignorebins = [(median_mean+(median_stddev*medianhighstd) < c) \n",
    "                      or median_mean-(median_stddev*medianlowstd) > c for c in medians]\n",
    "    else:\n",
    "        medianignorebins = [False for c in range(len(matrix[0]))]        \n",
    "\n",
    "    \n",
    "    \n",
    "    if \"sums\" in correction:\n",
    "        sums = np.sum(matrix,axis=0) + np.sum(matrix,axis=1)\n",
    "        sum_mean = np.mean(sums[sums != 0])\n",
    "        sum_stddev = np.std(sums[sums != 0])\n",
    "        print(\"Sum Mean: {0}\".format(sum_mean))\n",
    "        print(\"Sum Std dev: {0}\".format(sum_stddev))\n",
    "        high_line_data = np.array([sum_mean+(sum_stddev*highstd) for i in range(len(sums))])\n",
    "        low_line_data = np.array([sum_mean-(sum_stddev*lowstd) for i in range(len(sums))])\n",
    "        sumymin = sum_mean+(sum_stddev*highstd*5)\n",
    "        sumymax = sum_mean-(sum_stddev*lowstd*5)\n",
    "        \n",
    "        if ignoreplots:\n",
    "            plt.plot(sums)\n",
    "            plt.ylim(sumymin,sumymax)\n",
    "            plt.plot(high_line_data, 'r--')\n",
    "            plt.plot(low_line_data, 'r--')\n",
    "            plt.show()\n",
    "        sumignorebins = [(sum_mean+(sum_stddev*highstd) < c) or sum_mean-(sum_stddev*lowstd) > c for c in sums]\n",
    "    else:\n",
    "        sumignorebins = [False for c in range(len(matrix[0]))]\n",
    "        \n",
    "    \n",
    "    if \"nonzero\" in correction:\n",
    "        nonzerocounts = np.count_nonzero(matrix,axis=0)\n",
    "        nonzero_mean = np.mean(nonzerocounts[nonzerocounts != 0])\n",
    "        nonzero_stddev = np.std(nonzerocounts[nonzerocounts != 0])\n",
    "        print(\"Nonzero Mean: {0}\".format(nonzero_mean))\n",
    "        print(\"Nonzero Std dev: {0}\".format(nonzero_stddev))\n",
    "        nonzeroignorebins = [(nonzero_mean+(nonzero_stddev*nz_high_threshold) < c) or\n",
    "                      (nonzero_mean-(nonzero_stddev*nz_low_threshold) > c) for c in nonzerocounts]    \n",
    "        if ignoreplots:\n",
    "            plt.plot(nonzerocounts)\n",
    "            high_line_data = np.array([nonzero_mean+(nonzero_stddev*nz_high_threshold) for i in range(len(nonzerocounts))])\n",
    "            low_line_data = np.array([nonzero_mean-(nonzero_stddev*nz_low_threshold) for i in range(len(nonzerocounts))])   \n",
    "            plt.plot(high_line_data, 'r--')\n",
    "            plt.plot(low_line_data, 'r--')\n",
    "            plt.show()        \n",
    "    else:\n",
    "        nonzeroignorebins = [False for c in range(len(matrix[0]))]  \n",
    "        \n",
    "    df = dfs[chrindex-chrstart]\n",
    "    \n",
    "    sigs = []\n",
    "    \n",
    "    #load in sigs so we can remove 0 quantiles\n",
    "    signames = df.columns[-comp_num:].tolist()\n",
    "    print(signames)\n",
    "    for index, row in df.iloc[:,-comp_num:].iterrows():\n",
    "        try:\n",
    "            sigs.append([int(r) for r in row][-comp_num:])\n",
    "        except ValueError:\n",
    "            print(\"NaN\")\n",
    "            print(index)\n",
    "            print(row)        \n",
    "    print('sigs')\n",
    "    #print(sigs)\n",
    "    \n",
    "    print(sigs[:10])\n",
    "    #here we reverse sigs\n",
    "    #signames.reverse()\n",
    "    #holdsigs = [s.reverse() for s in sigs]\n",
    "    #sigs = holdsigs\n",
    "    #for s in sigs:\n",
    "    #    s.reverse()\n",
    "    #print(signames)\n",
    "    #print(sigs[:10])\n",
    "    \n",
    "    \n",
    "    if \"quant\" in correction:\n",
    "        quantignorebins = [False for c in range(len(matrix[0]))]\n",
    "        for n in range(len(signames)):\n",
    "            print()\n",
    "            for x in range(len(sigs)):\n",
    "                if sigs[x][n] == 0:\n",
    "                    quantignorebins[x] = True\n",
    "    else:\n",
    "        quantignorebins = [False for c in range(len(matrix[0]))]      \n",
    "    \n",
    "    ignorebins = [w | x | y | z for (w,x,y,z) in zip(medianignorebins, sumignorebins, nonzeroignorebins,quantignorebins)]\n",
    "    print(\"Ignoring {0} median bins\".format(sum(medianignorebins)))\n",
    "    print(\"Ignoring {0} sum bins\".format(sum(sumignorebins)))\n",
    "    print(\"Ignoring {0} zero bins\".format(sum(nonzeroignorebins)))\n",
    "    print(\"Ignoring {0} quant bins\".format(sum(quantignorebins)))\n",
    "\n",
    "    #now we need to ignore z-norm outliers\n",
    "    #first we set ignore rows and columns to nan\n",
    "    #so they will be omitted by zscore operation\n",
    "    nan_ignore_matrix = np.copy(matrix)\n",
    "    nan_ignore_matrix[ignorebins,:] = np.nan\n",
    "    nan_ignore_matrix[:,ignorebins] = np.nan\n",
    "    \n",
    "    #print(nan_ignore_matrix[0:10,0:10])\n",
    "    \n",
    "    \n",
    "    zscoreignorebins = np.zeros(sizebins, dtype=bool)\n",
    "    for d in range(sizebins):\n",
    "        zscores = stats.zscore(np.diag(nan_ignore_matrix,d),nan_policy='omit')\n",
    "        for e,z in enumerate(zscores):\n",
    "            if z == np.nan:\n",
    "                zscoreignorebins[e] = False\n",
    "                zscoreignorebins[e+d] = False\n",
    "            elif z > zscore_threshold:\n",
    "                #make both row and column false\n",
    "                zscoreignorebins[e] = True\n",
    "                zscoreignorebins[e+d] = True\n",
    "                \n",
    "    #now we remove these bad bins\n",
    "    print(\"Ignoring {0} zscore bins\".format(sum(zscoreignorebins)))\n",
    "    ignorebins = [x | y for (x,y) in zip(ignorebins,zscoreignorebins)]\n",
    "    \n",
    "    print(\"Ignoring {0} total bins\".format(sum(ignorebins)))\n",
    "    #now matrix is our entirely unnormalized matrix\n",
    "    #initialize our variables:\n",
    "    dists = list(D[:sizebins])\n",
    "    maps = list(M[:])\n",
    "    print(\"len of dists\")\n",
    "    print(len(dists))\n",
    "    \n",
    "\n",
    "    #for col in df.columns[:comp_num]:\n",
    "    #    comps.append(list(df[col][:sizebins]))\n",
    "    \n",
    "    \n",
    "\n",
    "    #for x in range(len(comps)):\n",
    "    #    for y in range(len(comps[x])):\n",
    "    #        if comps[x][y] < 0:\n",
    "    #            comps[x][y] = 0\n",
    "    #print(len(sigs[0]))\n",
    "    #print(len(maps))\n",
    "    \n",
    "    #here we want to set True on ignore flag for any bins outside sig coverage\n",
    "    #print(\"sigs\")\n",
    "    #print(len(sigs))\n",
    "    #print(\"ignore\")\n",
    "    #print(len(ignorebins))\n",
    "\n",
    "    if len(sigs) < len(ignorebins):\n",
    "        for x in range(len(sigs),len(ignorebins)):\n",
    "            #print(x)\n",
    "            ignorebins[x] = True\n",
    "    \n",
    "    LLL = np.Inf\n",
    "    count = 0\n",
    "    error = 100000\n",
    "    #d_error = 100\n",
    "    #b_error = 100\n",
    "    #c_error = 100\n",
    "    \n",
    "    #OK now let's define all our matrices:\n",
    "    #to get y we need to compress to remove ignore bins, flatten,\n",
    "    igno = np.where(ignorebins)[0]\n",
    "    #print(igno)\n",
    "    matrixdf = pd.DataFrame(matrix)\n",
    "    #print(\"matrix\")\n",
    "    #print(matrixdf.shape)\n",
    "    #matrixdf.drop(igno,axis=0,inplace=True)\n",
    "    #matrixdf.drop(igno,axis=1,inplace=True)\n",
    "    #plt.imshow(matrixdf,vmin=0,vmax=100)\n",
    "    #plt.show()\n",
    "    #print(ignorebins)\n",
    "    #matrixdf.iloc[:,igno] = 0\n",
    "    #matrixdf.iloc[igno,:] = 0    \n",
    "    matrixdf.iloc[:,ignorebins] = np.nan\n",
    "    matrixdf.iloc[ignorebins,:] = np.nan\n",
    "    #plt.imshow(matrixdf,vmin=0,vmax=100)\n",
    "    #plt.show()\n",
    "    #now nan out the lower triangle:\n",
    "    matrixdf = matrixdf.where(np.tril(np.triu(np.ones(matrixdf.shape),k=1+distance_min_bins),k=min(distance_max_bins,matrixdf.shape[0])).astype(np.bool))\n",
    "    matrixdf = matrixdf.stack().reset_index()\n",
    "    matrixdf.columns = ['i','j','v']\n",
    "    #print(matrixdf)\n",
    "    Y = matrixdf['v']\n",
    "    #Y = np.reshape(np.array(Y),(Y.shape[0],1))\n",
    "    \n",
    " \n",
    "    #y = matrixdf.to_numpy.flatten()#observed values vector\n",
    "    #B is now the learned coefficients. Basically the flattened maps. Constructed by comparing sigs to y\n",
    "    #tiles * tiles * comp_num\n",
    "    ttiles = tiles-1\n",
    "    trisize = ((ttiles*(ttiles+1))//2)\n",
    "    #print(\"sizes\")\n",
    "    #print(trisize)\n",
    "    bsize = trisize*comp_num\n",
    "    #print(bsize)\n",
    "    B = np.zeros(bsize)\n",
    "    #B[0] = 1\n",
    "    #B = np.reshape(B,(bsize,1))\n",
    "    \n",
    "    #print('ysize')\n",
    "    #print(Y.shape)\n",
    "    #print('bsize')\n",
    "    #print(bsize)    \n",
    "    \n",
    "    \n",
    "    #lastly we need to define H. This is the expected value based on distance for every value.\n",
    "    #we can initialize it as 0's and then learn it first.\n",
    "    H = np.zeros(Y.shape[0])\n",
    "    #H = np.reshape(H,(Y.shape[0],1))\n",
    "    #to learn H you learn the full matrix, then you average every distance together\n",
    "    H_dists = matrixdf['j'] - matrixdf['i']\n",
    "    #print(H_dists)\n",
    "    #print(H_dists)\n",
    "    #print(H_dists[H_dists==1])\n",
    "    Hadj = np.ones(len(dists))\n",
    "    Hadj_exp = np.ones(Y.shape[0])\n",
    "    \n",
    "    #X is a CSR sparse matrix with each column corresponding to a sig-tile and each row a matrix bin.\n",
    "    #X dimensions are y x B\n",
    "    #once we have flattened y we can fill in each element in X. Should be 4 1's per row \n",
    "    #we construct by iterating over matrixdf, grabbing the appropriate sigs and adding that to a COO or DOK matrix.\n",
    "    xijdic = {}\n",
    "    for x in range(comp_num):\n",
    "        for i in range(ttiles):\n",
    "            for j in range(i,ttiles):\n",
    "                y = trisize*x + np.sum(np.array(range(-1*i,0))+ttiles+1) + j-i\n",
    "                xijdic[y] = [x,i,j]\n",
    "    \n",
    "    ii = []\n",
    "    jj = []\n",
    "    dd = []\n",
    "    for index, row in matrixdf.iterrows():\n",
    "        #first start with 1 in every row for B_0:\n",
    "        #ii.append(index)\n",
    "        #jj.append(0)\n",
    "        #dd.append(1)\n",
    "        #print(row)\n",
    "        #print(row['i'])\n",
    "        #print(row['j'])\n",
    "        sigs_i = sigs[int(row['i'])]\n",
    "        sigs_j = sigs[int(row['j'])]\n",
    "        for x, (i,j) in enumerate(zip(sigs_i,sigs_j)):\n",
    "            #here x is comp num, i is first quantile, j is second quantile\n",
    "            #formula to get correct bin therefore is (1+tiles*tiles*x + tiles*i + j)\n",
    "            if i > j:\n",
    "                h = i\n",
    "                i = j\n",
    "                j = h\n",
    "            ii.append(index)\n",
    "            i = i-1\n",
    "            j = j-1\n",
    "            #jj.append(1+tiles*tiles*x + tiles*i + j)\n",
    "            y = trisize*x + np.sum(np.array(range(-1*i,0))+ttiles+1) + j-i\n",
    "            jj.append(y)\n",
    "            dd.append(1)\n",
    "        \n",
    "        \n",
    "    #print(xijdic)\n",
    "    coo = coo_matrix((dd, (ii, jj)), shape=(Y.shape[0], bsize))\n",
    "    #print(coo_matrix)\n",
    "    X = csr_matrix(coo)\n",
    "    #print('X')\n",
    "    #print(np.sum(X,axis=0))\n",
    "    #print(np.sum(X,axis=1))\n",
    "    \n",
    "    for d in range(1, len(dists)):\n",
    "        #d_indices = np.where(H_dists == d)\n",
    "        dmean = np.mean(Y[H_dists == d])\n",
    "        H[H_dists == d] = dmean\n",
    "        dists[d] = dmean\n",
    "\n",
    "    #print(dists)\n",
    "    #H = H1\n",
    "\n",
    "    plt.plot(dists)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.title(\"Distance Vector {0}\".format(chrindex))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    #let's compute the distance parameter\n",
    "    #print(len(sigs))\n",
    "    #print(len(ignorebins))\n",
    "    #print(ignorebins[-10:])\n",
    "    \n",
    "    print(\"X {0}\".format(X.shape))\n",
    "    print(\"Y {0}\".format(Y.shape))\n",
    "    print(\"B {0}\".format(B.shape))\n",
    "    print(\"H {0}\".format(H.shape))\n",
    "    \n",
    "\n",
    "\n",
    "    #plt.plot(dists)\n",
    "    #plt.title(\"Distance Vector {0}\".format(chrindex))\n",
    "    #plt.show()\n",
    "    #now we will begin iterating\n",
    "    while error > tolerance and count < max_iter:\n",
    "        #while count < max_iter:\n",
    "        count += 1\n",
    "        \n",
    "        #olddists = np.copy(dists)\n",
    "        #for d in range(len(dists)):\n",
    "        #    dists[d] = ncomp_distance_update(d,matrix,sizebins,dists,sigs,maps,distance_ranges,ignorebins)\n",
    "        #olddists = dists\n",
    "        #plt.plot(olddists)\n",
    "        #XB = (X @ B)\n",
    "        #print(XB.shape)\n",
    "        #print(XB)\n",
    "        #H1 = Y/(X @ B)\n",
    "        #print(H1)\n",
    "        for d in range(1,len(dists)):\n",
    "            #d_indices = H_dists[H_dists==d]\n",
    "            XBmean = np.mean(mu(X,B)[H_dists==d])\n",
    "            Hadj_exp[H_dists==d] = XBmean\n",
    "            Hadj[d] = XBmean\n",
    "        \n",
    "        \n",
    "        plt.plot(Hadj)\n",
    "        plt.show()\n",
    "        #now divide by H\n",
    "        Yh = np.nan_to_num(Y/(H / Hadj_exp))\n",
    "        #print(len(H))\n",
    "        #print(len(Hadj_exp))\n",
    "        #print(len(Y))\n",
    "        #print(len(Yh))\n",
    "        #print(Yh)\n",
    "        print(\"len of dists\")\n",
    "        print(len(dists))\n",
    "        plt.plot(dists)\n",
    "        plt.plot(dists / Hadj)\n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        plt.show()\n",
    "        \n",
    "        #now we update B\n",
    "        lamb = 0.1\n",
    "        #G = grad(Y,X,B,H)\n",
    "        #u  = H * (X @ B)\n",
    "        #print(\"u\")\n",
    "        #print(u.shape)\n",
    "        #G = X.T @ ((Y * H) / u) - X.T @ H\n",
    "        Grad = grad(Yh,X,B)\n",
    "        #He = hess(Y,X,B,H)\n",
    "        #print(Grad.shape)\n",
    "        #iGrad = np.linalg.inv(Grad)\n",
    "        \n",
    "        #u2 = (X @ B) * (X @ B)\n",
    "        \n",
    "        #yu2 = Y/u2\n",
    "        \n",
    "        #print(yu2.shape)\n",
    "        #print(X.shape)\n",
    "        \n",
    "        #diag_i = []\n",
    "        #diag_j = []\n",
    "        #diag_v = []\n",
    "        #for x,entry in enumerate(yu2.flatten()):\n",
    "        #    diag_i.append(x)\n",
    "        #    diag_j.append(x)\n",
    "        #    diag_v.append(entry)\n",
    "            \n",
    "        #coo_diag = coo_matrix((diag_v, (diag_i, diag_j)), shape=(Y.shape[0], Y.shape[0]))\n",
    "        #print(coo_diag)\n",
    "        #csr_diag = csr_matrix(coo_diag)\n",
    "        \n",
    "        #print('csr')\n",
    "        #print(csr_diag.shape)\n",
    "        #print(yu2.shape)\n",
    "        #print(X.shape)\n",
    "        \n",
    "        #xyu2 = csr_diag @ X\n",
    "        #xyu2 = yu2.flatten() * X \n",
    "        #xyu2 = X @ yu2\n",
    "        #print(\"xyu2\")\n",
    "        #print(xyu2.shape)\n",
    "        \n",
    "        #xxyu2 = xyu2.T @ X\n",
    "        \n",
    "        #print(xxyu2.shape)\n",
    "        \n",
    "        #Hess = -1 * (Y / ((X @ B) * (X @ B)) * X).T @ X\n",
    "        #Hess = -1 * xxyu2.todense()\n",
    "        Hess = hess(Yh,X,B).todense()\n",
    "        #temp = (Y.flatten() / ((X.dot(B.flatten())) * (X.dot(B.flatten()))))[:, np.newaxis]\n",
    "        #print('temp')\n",
    "        #print(temp.shape)\n",
    "        #print(X.shape)\n",
    "        #Hess = -((temp * X).T).dot(X)\n",
    "        \n",
    "        #print(Hess.shape)\n",
    "        #print(Hess)\n",
    "        plt.imshow(Hess)\n",
    "        plt.title('Hessian')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        #plt.imshow(np.linalg.inv(Hess))\n",
    "        #plt.title('inverse')\n",
    "        #plt.colorbar()\n",
    "        plt.show()\n",
    "        plt.imshow(np.linalg.pinv(Hess))\n",
    "        plt.title('pseudoinverse')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        #B_new = B - (np.linalg.pinv(Hess) @ Grad)\n",
    "        lamb = 1/max(np.absolute(Grad))\n",
    "        #B_new = B + lamb*Grad\n",
    "        inv = np.linalg.pinv(Hess)\n",
    "        #print(inv.shape)\n",
    "        step = -1 * (inv @ Grad[:, np.newaxis])\n",
    "        #step = 1 * Grad * lamb\n",
    "        #print(step.shape)\n",
    "        #print(B.shape)\n",
    "        B_new = B[:, np.newaxis] - step\n",
    "        #B_new = B + step\n",
    "        #print(B_new.shape)\n",
    "        B = np.array(B_new).reshape(B.shape[0])\n",
    "        #B = np.array(B_new)\n",
    "        #print(Grad.shape)\n",
    "        #print(B.shape)\n",
    "        #here we should reshape B to visualize as 4 heatmaps.\n",
    "        #inverse of this operation: 1+trisize*x + np.sum(np.array(range(-1*i,0))+tiles+1) + j\n",
    "        G_reshape = np.zeros((comp_num,ttiles,ttiles))\n",
    "        #print(\"G_0\")\n",
    "        #print(Grad[0])\n",
    "        for n,g in enumerate(Grad):\n",
    "            x,i,j = xijdic[n]\n",
    "            G_reshape[x,i,j] = g\n",
    "            G_reshape[x,j,i] = g\n",
    "        \n",
    "        #G_reshape = np.reshape(Grad[1:],(comp_num,tiles,tiles))\n",
    "        #for g,n in zip(G_reshape,signames):\n",
    "        #    sns.heatmap(g,center=0)\n",
    "        #    plt.title(\"Gradient \"+ n)\n",
    "        #    plt.show()\n",
    "            \n",
    "        S_reshape = np.zeros((comp_num,ttiles,ttiles))\n",
    "        #print(\"G_0\")\n",
    "        #print(Grad[0])\n",
    "        for n,s in enumerate(step):\n",
    "            x,i,j = xijdic[n]\n",
    "            S_reshape[x,i,j] = s\n",
    "            S_reshape[x,j,i] = s\n",
    "        \n",
    "        #G_reshape = np.reshape(Grad[1:],(comp_num,tiles,tiles))\n",
    "        #print(len(S_reshape))\n",
    "        #for s,n in zip(S_reshape,signames):\n",
    "        #    sns.heatmap(s,center=0)\n",
    "        #    plt.title(\"Step \"+ n)\n",
    "        #    plt.show()\n",
    "            \n",
    "        B_reshape = np.zeros((comp_num,ttiles,ttiles))\n",
    "        #print(\"B_0\")\n",
    "        #print(B[0])\n",
    "        for n,b in enumerate(B):\n",
    "            x,i,j = xijdic[n]\n",
    "            B_reshape[x,i,j] = b\n",
    "            B_reshape[x,j,i] = b\n",
    "        #B_reshape = np.reshape(B[1:],(comp_num,tiles,tiles))\n",
    "        for b,n in zip(B_reshape,signames):\n",
    "            sns.heatmap(b,center=0)\n",
    "            plt.title(\"Maps \"+ n)\n",
    "            plt.show()\n",
    "            \n",
    "        \n",
    "        \n",
    "        #now update the maps\n",
    "        #maps,counts = ncomp_mapping_update(matrix,sigs,maps,tiles,dists,distance_indices,slims,ignorebins,distance_min_bins)\n",
    "\n",
    "        #print(maps)\n",
    "        #print(counts)\n",
    "\n",
    "        #for n,m,c in zip(signames,maps,counts):\n",
    "        #    sns.heatmap(m,center=0)\n",
    "        #    plt.title(\"Maps \" + n)\n",
    "        #    plt.show()\n",
    "        #    #sns.heatmap(c)\n",
    "        #    #plt.title(\"Counts \" + n)\n",
    "        #    #plt.show()\n",
    "\n",
    "        #now let's compute log likelihood for graphing and testing purposes\n",
    "\n",
    "        #LL = 0\n",
    "\n",
    "        vis = np.zeros((sizebins,sizebins))\n",
    "        visc = np.zeros((sizebins,sizebins))\n",
    "        \n",
    "        #LL = logL(Y,X,B,H)\n",
    "        \n",
    "        #u = mu(X,B,H)\n",
    "        #logu = np.log(u)\n",
    "        #print(np.any(np.isinf(u)))\n",
    "        #print(np.any(np.isposinf(u)))\n",
    "        #print(np.any(np.isneginf(u)))\n",
    "        #LL = np.nansum(Y * logu - u)\n",
    "        \n",
    "        LL = logL(Yh,X,B)\n",
    "        \n",
    "        #plt.imshow(np.log(vis))\n",
    "        #plt.show()     \n",
    "\n",
    "        print(\"Log likelihood: {0}\".format(LL))\n",
    "        print(\"Last log likelihood: {0}\".format(LLL))\n",
    "        error = LLL-LL\n",
    "        print(\"Chromosome {0} Iteration {1} Improvement: {2}\".format(chrom1,count,error))\n",
    "        LLL = LL\n",
    "        \n",
    "        \n",
    "        expected = mu(X,B)\n",
    "        observed = Yh\n",
    "        expectedc = mu(X,B) * (H / Hadj_exp)\n",
    "        observedc  = Y\n",
    "        #print(observed)\n",
    "        #print(expected)\n",
    "        pearson = np.corrcoef(observed,expected)[0,1]\n",
    "        print('Pearson: (0)'.format(pearson))\n",
    "        \n",
    "        for o,e,i,j in zip(observed,expected,matrixdf['i'],matrixdf['j']):\n",
    "            vis[i,j] = e\n",
    "            vis[j,i] = o\n",
    "        for o,e,i,j in zip(observedc,expectedc,matrixdf['i'],matrixdf['j']):\n",
    "            visc[i,j] = e\n",
    "            visc[j,i] = o            \n",
    "            \n",
    "        matplotlib.rcParams['figure.figsize'] = (12,12)\n",
    "        plt.imshow(vis)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(vis[100:200,100:200])\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        matplotlib.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "        matplotlib.rcParams['figure.figsize'] = (12,12)\n",
    "        plt.imshow(visc,vmin=0,vmax=1000)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(visc[100:200,100:200],vmin=0,vmax=1000)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        matplotlib.rcParams['figure.figsize'] = (6,6)\n",
    "        #plt.plot(dists)\n",
    "        #plt.title(\"Distance Vector {0}\".format(chrindex))\n",
    "        #plt.show()\n",
    "        #plt.imshow(matrix)\n",
    "        #plt.show()\n",
    "        #plt.plot(bias)\n",
    "        #plt.title(\"Bias Vector {0}\".format(chrindex))\n",
    "        #plt.show()\n",
    "        #plt.plot(comps)\n",
    "        #plt.title(\"C-Score {0}\".format(chrindex))\n",
    "        #plt.show()\n",
    "        maps = B_reshape\n",
    "        adj_dists = dists/Hadj\n",
    "        print(\"len of dists\")\n",
    "        print(len(adj_dists))\n",
    "    return (maps, sigs, adj_dists, ignorebins)\n",
    "\n",
    "\n",
    "#chrindex = 10\n",
    "#cs = MLE(chrindex,D,B,C,chrsizebins,distance_ranges,distance_indices,distance_min_bins,tolerance,ignore_threshold,max_iter)\n",
    "#print(cs)\n",
    "\n",
    "from ipyparallel import Client\n",
    "\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "\n",
    "#rc = Client()\n",
    "#print(rc.ids)\n",
    "\n",
    "\n",
    "#dv = rc[:]\n",
    "\n",
    "#var_dict = ('chrnames'=chrnames,D,B,C,chrsizebins,distance_ranges,distance_indices,distance_min_bins,\n",
    "#tolerance,ignore_threshold,max_iter),range(chrstart,chrstop))\n",
    "#dv.push({'MLE_ncomp': MLE_ncomp,\"ncomp_mapping_update\":ncomp_mapping_update,\n",
    "#        \"mapping\":mapping,\"ncomp_distance_update\":ncomp_distance_update})\n",
    "\n",
    "\n",
    "#lv = rc.load_balanced_view()\n",
    "\n",
    "msdi = []\n",
    "for x in range(chrstart,chrstop):\n",
    "    msdi.append(MLE_ncomp(x))\n",
    "\n",
    "#msdi = dv.map_sync(lambda chrindex: MLE_ncomp(chrindex), range(chrstart,chrstop))\n",
    "#print(cdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to unpack c,d,b\n",
    "\n",
    "\n",
    "mappings = []\n",
    "signals = []\n",
    "distances = []\n",
    "ignored = []\n",
    "\n",
    "for chromosome in msdi:\n",
    "    mappings.append(chromosome[0])\n",
    "    signals.append(chromosome[1])\n",
    "    distances.append(chromosome[2])\n",
    "    ignored.append(chromosome[3])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfnames = []\n",
    "for c in dfs[0]:\n",
    "    dfnames.append(c)\n",
    "\n",
    "#temp\n",
    "#we use names\n",
    "    \n",
    "#here let's export tile thresholds and mappings to file\n",
    "for chrindex in range(chrstart,chrstop):\n",
    "    corrindex = chrindex - chrstart\n",
    "    #for x,signame in enumerate(dfnames[-comp_num:]):\n",
    "    for x,signame in enumerate(dfnames[-comp_num:]):\n",
    "        mapfileoutname = \"/Zulu/mike/learnedmaps/{0}-{1}kb-chr{2}-{3}-learnedmaps-min:max-{4}kb:{5}kb.csv\".format(celltype,resolution//1000,chrnames[chrindex],signame,distance_min//1000,distance_max//1000)\n",
    "        #mapfileoutname = \"/Zulu/mike/learnedmaps/{0}-{1}kb-chr{2}-{3}-tiles-learnedmaps-mindist-{4}kb.csv\".format(celltype,resolution//1000,chrnames[chrindex],signame,distance_min//1000)\n",
    "\n",
    "        mapdf = pd.DataFrame(mappings[corrindex][x])\n",
    "        mapdf.to_csv(mapfileoutname,index=False)\n",
    "        print(mapfileoutname)\n",
    "        \n",
    "    distancefileoutname = \"/Zulu/mike/learneddistances/{0}-{1}kb-chr{2}-distances-min:max-{3}kb:{4}kb.csv\".format(celltype,resolution//1000,chrnames[chrindex],distance_min//1000,distance_max//1000)\n",
    "    distancedf = pd.DataFrame(distances[chrindex])\n",
    "    distancedf.to_csv(distancefileoutname,index=False,na_rep=0)\n",
    "    print(distancefileoutname)\n",
    "    print(distancedf)\n",
    "    ignorefileoutname = \"/Zulu/mike/learnedignores/{0}-{1}kb-chr{2}-ignore.csv\".format(celltype,resolution//1000,chrnames[chrindex])\n",
    "    ignoredf = pd.DataFrame(ignored[corrindex])\n",
    "    ignoredf.to_csv(ignorefileoutname,index=False)\n",
    "    print(ignorefileoutname)\n",
    "    #ignorefileoutname = \"{0}-{1}kb-chr{2}-ignore.csv\".format(celltype,resolution//1000,chrnames[chrindex])\n",
    "    #ignoredf = pd.DataFrame(ignored[corrindex])\n",
    "    #ignoredf.to_csv(ignorefileoutname,index=False)\n",
    "    #print(ignorefileoutname)        \n",
    "#! head $ignorefileoutname\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(chrstart,chrstop):\n",
    "    plt.plot(distances[x])\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (14,7)\n",
    "\n",
    "#for maps in mappings:\n",
    "#        for n,m in zip(dfs[0],maps):\n",
    "#            sns.heatmap(m,center=0)\n",
    "#            plt.title(n)\n",
    "#            plt.show()\n",
    "\n",
    "#for x in range(len(mappings)):\n",
    "\n",
    "\n",
    "    \n",
    "#print(dfnames[:comp_num])\n",
    "for n,m1,m2 in zip(dfnames[-comp_num:],mappings[5],mappings[6]):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "    #sns.heatmap((m1[:19,:19]), center=0, vmin =-1,vmax=1, ax=ax1)\n",
    "    #sns.heatmap((m1[:19,:19]-(m1[:19,:19].mean()))/m1[:19,:19].std(ddof=1), center=0, vmin =-4,vmax=4, ax=ax1)\n",
    "    #sns.heatmap((m1[:19,:19]-(m1[:19,:19].mean())), center=0, vmin =-1,vmax=1, ax=ax1)\n",
    "    sns.heatmap((m1[:tiles,:tiles]), center=0, vmin =-1,vmax=1,ax=ax1)\n",
    "    plt.title(n)\n",
    "    #sns.heatmap((m2[:19,:19]), center=0, vmin =-1,vmax=1, ax=ax2)\n",
    "    #sns.heatmap((m2[:19,:19]-(m2[:19,:19].mean()))/m2[:19,:19].std(ddof=1), center=0, vmin =-4,vmax=4, ax=ax2)\n",
    "    #sns.heatmap((m2[:19,:19]-(m2[:19,:19].mean())), center=0, vmin =-1,vmax=1, ax=ax2)\n",
    "    sns.heatmap((m2[:tiles,:tiles]), center=0, vmin =-1,vmax=1, ax=ax2)\n",
    "    plt.title(n)\n",
    "    plt.show()\n",
    "    \n",
    "#print(m1[:tiles,:tiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
